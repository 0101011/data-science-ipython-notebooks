{
 "metadata": {
  "name": "",
  "signature": "sha256:cbe332e49ee1a97b7e73961a5b49d8f32b4b13da36145a34bb8ecb72d5843320"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# AWS Command Lines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Connect to EC2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connect to an Ubuntu EC2 instance through SSH with the given key:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ssh -i key.pem ubuntu@ipaddress"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connect to an Amazon Linux EC2 instance through SSH with the given key:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ssh -i key.pem ec2-user@ipaddress"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## S3DistCp"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To run S3DistCp with the EMR command line, ensure you are using the proper version of Ruby:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rvm --default ruby-1.8.7-p374"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The EMR command line below executes the following:\n",
      "* Create a master node and slave nodes of type m1.small\n",
      "* Runs S3DistCp on the source bucket location and concatenates files that match the date regular expression, resulting in files that are roughly 1024 MB or 1 GB\n",
      "* Places the results in the destination bucket"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "./elastic-mapreduce --create --instance-group master --instance-count 1 \\\n",
      "--instance-type m1.small --instance-group core --instance-count 4 \\\n",
      "--instance-type m1.small --jar /home/hadoop/lib/emr-s3distcp-1.0.jar \\\n",
      "--args \"--src,s3://my-bucket-source/,--groupBy,.*([0-9]{4}-01).*,\\\n",
      "--dest,s3://my-bucket-dest/,--targetSize,1024\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For further optimization, compression can be helpful to save on AWS storage and bandwidth costs, to speed up the S3 to/from EMR transfer, and to reduce disk I/O. Note that compressed files are not easy to split for Hadoop. For example, Hadoop uses a single mapper per GZIP file, as it does not know about file boundaries.\n",
      "\n",
      "What type of compression should you use?\n",
      "\n",
      "* Time sensitive job: Snappy or LZO\n",
      "* Large amounts of data: GZIP\n",
      "* General purpose: GZIP, as it\u2019s supported by most platforms\n",
      "\n",
      "You can specify the compression codec (gzip, lzo, snappy, or none) to use for copied files with S3DistCp with \u2013outputCodec. If no value is specified, files are copied with no compression change. The code below sets the compression to lzo:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "--outputCodec,lzo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },

    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Setup S3cmd"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Install s3cmd:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sudo apt-get install s3cmd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure s3cmd:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s3cmd --configure"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Frequently used S3cmds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List all buckets\n",
      "s3cmd ls\n",
      "\n",
      "# List the contents of the bucket\n",
      "s3cmd ls s3://my-bucket-name\n",
      "\n",
      "# Upload a file into the bucket (private)\n",
      "s3cmd put myfile.txt s3://my-bucket-name/myfile.txt\n",
      "\n",
      "# Upload a file into the bucket (public)\n",
      "s3cmd put --acl-public --guess-mime-type myfile.txt s3://my-bucket-name/myfile.txt\n",
      "\n",
      "# Recursively upload a directory to s3\n",
      "s3cmd put --recursive my-local-folder-path/ s3://my-bucket-name/mydir/\n",
      "\n",
      "# Download a file\n",
      "s3cmd get s3://my-bucket-name/myfile.txt myfile.txt\n",
      "\n",
      "# Recursively download files that start with myfile\n",
      "s3cmd --recursive get s3://my-bucket-name/myfile\n",
      "\n",
      "# Delete a file\n",
      "s3cmd del s3://my-bucket-name/myfile.txt\n",
      "\n",
      "# Delete a bucket\n",
      "s3cmd del --recursive s3://my-bucket-name/\n",
      "\n",
      "# Create a bucket\n",
      "s3cmd mb s3://my-bucket-name\n",
      "\n",
      "# List bucket disk usage (human readable)\n",
      "s3cmd du -H s3://my-bucket-name/\n",
      "\n",
      "# Sync local (source) to s3 bucket (destination)\n",
      "s3cmd sync my-local-folder-path/ s3://my-bucket-name/\n",
      "\n",
      "# Sync s3 bucket (source) to local (destination)\n",
      "s3cmd sync s3://my-bucket-name/ my-local-folder-path/\n",
      "\n",
      "# Do a dry-run (do not perform actual sync, but get information about what would happen)\n",
      "s3cmd --dry-run sync s3://my-bucket-name/ my-local-folder-path/\n",
      "\n",
      "# Apply a standard shell wildcard include to sync s3 bucket (source) to local (destination)\n",
      "s3cmd --include '2014-05-01*' sync s3://my-bucket-name/ my-local-folder-path/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}